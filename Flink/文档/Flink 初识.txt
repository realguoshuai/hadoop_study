# 流数据 #
什么是流数据 ?
从广义上说,所有大数据的生成均可以看作是一连串发生的离散事件.
这些离散的事件以时间轴为维度看就形成了一条条事件流/数据流
数据是指由数千个数据源持续生成的数据,流数据通常也以数据记录的形式发送,
但相较于离线数据,流数据普遍的规模较小.

# Flink #
Apache Flink 是一个分布式的大数据处理引擎,可对有限数据流和无线数据流
进行有状态或无状态的的计算.
优势:
Flink具备7X24小时高可用的SOA(面向服务架构),原因是在实现上 Flink 提供了一致性的 Checkpoint
Checkpoint 是 Flink 实现容错机制的核心,它周期性的记录计算过程中 Operator 的状态,并生成快照持久化存储
当 Flink 作业发生故障崩溃时,可以有选择的从 Checkpoint 中恢复,保证了计算的一致性

### 实时计算应用场景 ####
# 实时ETL
实时ETL&数据流的目的是实时的把数据从A点投递到B点.
中间可能会加上一些数据清洗和集成的工作,比如实时构建搜索系统的索引,实时数仓中的ETL过程等
eg:尾号限行  (对违反限行车辆进行初步筛选 )

# 实时数据分析(实时汇总)
数据分析指的是根据业务目标从原始数据中抽取对应信息并整合的过程.
比如查看每天卖的最好的十种商品,仓库平均周转时间,文章平均点击率,推送打开率等等.
实时数据分析则是上述过程的实时化,一般最终体现为实时报表或实时大屏
eg:30s流量统计 高频车 重点路段/区域  流量/拥堵

# 事件驱动应用  (实时预警)
事件驱动应用是对一系列订阅事件进行处理或作出响应的系统.
事件驱动应用往往还会依赖内部状态,比如点击欺诈检测,风控系统,运维异常检测系统等
当用户的行为触发某些风险控制点时,系统会捕获这个事件,并根据当前行为和用户之前的行为进行分析,
决定是否对用户进行风险控制
eg: 套牌车  流量突发预警   突发拥堵

### Flink Application Flink应用开发 相关概念 ####

Stream 流
分为有界数据流(bounded stream) && 无界数据流(unbounded stream)
两者区别:无界数据流的数据会随着时间的推移而持续增加,计算持续进行且不存在结束的状态  7x24   实时
		 有界数据流数据大小固定,计算最终会完成并处于结束的状态   离线

State  状态
是计算过程中的数据信息,在容错恢复和checkpoint中有重要作用
流计算在本质上是增量处理 (Incremental Processing),因此需要不断查询保持状态
同时,为了保持精确一次(Exactly-once)语义,需要数据能写入到状态中
同时持久化存储,能够保证在整个分布式系统运行失败或挂掉的情况下做到Exactly- once


Time   时间
事件时间,摄取时间,处理时间
Flink的无限数据流是一个持续的过程,时间是我们判断业务状态是否滞后,数据处理是否及时的依据


Window:
数据流是无限的,无界限. 但是可以通过一个有界的范围来处理无界的数据流.
滚动窗口:每个界限计算结果互不影响(不重合) 比如几分钟统计一次
滑动窗口:每个计算窗口统计数据是有重复的  每30s统计过去一分钟的过车数量



############    流处理如何解释时间??   #############
Time

Timer
定时器,作为window的触发源,分为两类:
WallTime Timer:按照正常的现实时间作为触发源
LowWatermark Timer:以低水位作为触发源 

low watermark :最低水位
其实就是一个时间戳 ,每一个计算节点都会维护一个时间戳作为watermark
A的低水位值不只和A本身的最旧数据有关,也跟上游的低水位有关.
因此,只要上游还有更旧的数据存在,就会通过低水位机制维护的low watermark告知下游,
下游便会更新它自己的low watermark并且由于lwm timer未触发，因此会进行等待

在一定程度上保证数据的完整性和实效性,但是如果有数据比lowwatermark还晚到达仍没有办法解决
比如:数据在没有进入流系统之前就耽搁了,那low watermark根本不知道
flink为了解决这个问题,还有allow lateness参数,即Window被low watermark timer触发后,
还会等待allow lateness时间才开始计算,但这样会损失一定的实时性

join:双流转换成单流
coGroup+innerjoin



#############   状态state和检查点checkpoint   ###########
How:最简单的wordcount,给一些word计算他们的count,count输出不断累加的结果,count就是状态
批处理对state要求不高:之前的批处理都是将数据分片计算,分片完成后做一个聚合,state要求较少
流处理程序,输入的是一个无限制的数据流,会运行很大一段时间,要求运行几天,几个月都不会宕机,
需要对中间的状态数据好好管理.storm采用的将状态数据存储到Hbase(计算时从hbase读,完了在更新到hbase,还需要保证数据一致性)

state可能会很大,阿里的几T都有

#########   flink的state   管理###########
按照数据划分和扩张方式:
Keyed States   Operator States 


checkpoint:程序指定时间定期生成 ,保留当前时间的算子的状态


savepoint:用户通过命令行触发,
	存储格式跟checkpoint不相同的 ,savepoint会按照一个标准的格式存储,不管配置什么样
	flink都会从这个checkpoint恢复,常用于版本升级
External Checkpoint:(外部checkpoint)做完一次checkpoint后在制定的目录中 
	多存储一份checkpoint 保留meta数据 双备份
	作业失败或取消状态结束时,外部存储的meta数据将保留下来

##State 和 checkpoint数据的存储方式:
MemoryStateBackend
FsStateBackend
RockDBStateBackend
数据量小存储在 MemoryStateBackend 和 FsStateBackend中
数据量大  存储在RockDBStateBackend 中

------------------
Flink是通过检查点的方式来实现 exactly-once 只执行一次,当遇到故障时将系统重置为初始状态







############流处理系统###############
包括 Flink 在内的分布式流处理引擎一般采用 DAG 图来表示整个计算逻辑,
DAG的每个点都代表一个基本的逻辑单元,即算子
Source接入-->网络传输/本地传输在算子间进行发送和处理-->sink发送到外部系统或数据库


######  基本模型 #######
逻辑模型  && 物理模型(运行时)

逻辑模型可能有多个并发,实际的分布式流处理引擎更复杂
,每个算子都可能有多个实例

假设: A B C三个算子,A source 有两个实例,C有两个实例,B 一个
在逻辑模型中,A和B是C的上游节点
而在对应的物理模型中,C的所有实例和A,B的所有实例之间可能都存在数据交换

# 注: 在物理模型中,我们会根据计算逻辑,采用系统自动优化或认为指定的方式将计算工作分布到不同的实例中,
只有当算子实例分布到不同进程时,才需要通过网络进行数据传输
,而同一进程中的 多个实例之间的数据传输通常不需要经过网络的

Storm&& Flink 构建DAG计算逻辑图区别?
Storm需要在图中添加Spout或Bolt这种算子,并指定算子之前的连接方式
Flink的Api定义更加面向数据本身的处理逻辑:
	将数据抽象成一个无限集,然后定义一组集合上的操作,最后在底层自动构建相应的DAG图
区别:Storm更底层,自由度高 Flink的Api更上层,也更简单

################### Flink DataStream API ################ 
//1、设置运行环境
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
	
//2、配置数据源读取数据	
DataStream<String> text = env.readTextFile ("input");
	
//3、进行一系列转换
DataStream<Tuple2<String, Integer>> counts = text.flatMap(new Tokenizer()).keyBy(0).sum(1);
	
//4、配置数据汇写出数据
counts.writeAsText("output");
	
//5、提交执行
env.execute("Streaming WordCount");

上面实现一个流式的wordcount,首先需要获得一个StreamExecutionEnvironment对象(构建DAG图的上下文对象)
,基于这个对象,我们可以添加一些算子
1:使用了 Environment 对象中内置的读取文件算子readTextFile获取数据源,获取到DataStream对象,它可以看做是一个无限的数据集
2:调用flatMap将每一条数据记录(文件中的每一行)分解成单词,同时会在底层的DAG图中添加一个flatMap算子
3:得到的是处理过的单词的流,调用keyBy算子将流中的单词分组/分流,然后调用sum算子进行累计
4:计算出的结果形成一个新的流,调用writeAsText算子将结果写到文件中 
5:只有最终调用execute方法时,才会把DAG提供到集群中,接入数据并执行实际的逻辑
  前面调用所有算子并没有实际处理数据,而是在构建表达计算逻辑的DAG图.
# 整个代码实现过程就是一个构建DAG 图的过程,将算子加到DAG 中 输入 处理 输出 


Flink DataStream API的核心就是代表流数据的DataStream对象,开发就是对DataStream对象进行操作
#########  DataStream操作分类:(四类)   ###########
1:基于单条记录(filter,map) 
2:基于窗口(window)  
3:合并多条流(union,join,connect)  
4:拆分单条流(split)

第一类是对于单条记录的操作.比如筛除掉不符合要求的记录(Filter 操作),或者将每条记录都做一个转换(Map 操作)
第二类是对多条记录的操作.比如说统计一个小时内的订单总成交量.就需要将一个小时内的所有订单记录的成交量加到一起
                         为了支持这种类型的操作,就得通过 Window 将需要的记录关联到一起进行处理
第三类是对多个流进行操作并转换为单个流.比如 多个流可以通过 Union、Join 或 Connect等操作合到一起
						这些操作合并的逻辑不同,但最终都会产生了一个新的统一的流,从而可以进行一些跨流的操作
第四类DataStream 还支持与合并对称的操作,就是把一个流按一定规则拆分为多个流(Split 操作)			 
						每个流是之前流的一个子集，这样我们就可以对不同的流作不同的处理


##########  DataStream转换  ############




 




---------------------------------------------------

## Flink Blob:
JobManager-blob服务 ,也是在flink-conf.yaml文件中配置 
	接收jar包/发送jar包到Taskmanager,传输log文件
## Flink 通信模型(via Akka)
Flink客户端(JobClient)  JobManager(1)  TaskManager(N) 之间通信都是基于Akka actor模型

JobClient 从用户处获取到Flink job,提交给JobManager
	1:JobManager 负责这个job的执行:首先分配所需的(slot:CPU,内存)资源,即TaskManagers上要执行的slot
	2:获取到slot后,jobmanager 部署单独的任务到响应的TaskManager上
	TaskManager产生一个线程来执行这个任务 
	3:状态改变时(开始计算,结束计算,每一次算子计算),状态会被发送回JobManager
	  基于这些状态的更新,JobManager将引导着个job执行完成
	4:一旦执行完,结果将会发送回JobClient

## JobManager和TaskManager
JobManager是核心控制单元,负责整个Flink Job,负责资源分配,任务调度和状态汇报
//TODO



flink  反压机制(backpressure)
产生原因:短时负载高峰导致系统接收数据的速率远高于它处理数据的速率
flink利用自身作为纯数据流引擎的优势来优雅地响应反压问题
Flink 是如何在 Task 之间传输数据的，以及数据流如何实现自然降速?
运行时: 
	operators组件  
		每个operator会消费中间态的流,并在流上进行转换,然后生成新的流
	streams组件

Flink中的反压:
	Flink 使用了高效有界的分布式阻塞队列,就像Java通用的阻塞队列(BlockingQueue)
		Java使用BlockingQueue时:一个较慢的接受者会降低发送者的发送速率,因为一旦队列
		满了(有界队列)发送者会被阻塞
在 Flink 中,这些分布式阻塞队列就是这些逻辑流,而队列容量是通过缓冲池来(LocalBufferPool)实现的
每个生产和被消费的流都会被分配一个缓冲池.
缓冲池管理着一组缓冲(Buffer),缓冲在被消费后可以被回收循环利用.

网络传输中的内存管理:

###################### Flink程序发布  ###################################

程序发布出问题: 
	1:程序发布找不到容器地址 Failed to retrieve JobManager address  2: 找不到日志路径 程序发布需要先创建log目录
1原因是在flink创建cluster后的的一定范围
nohup bin/yarn-session.sh \
--container 5 \
--jobManagerMemory 4024 \
--taskManagerMemory 4048 \
--slots 10 \
--jar $FLINK_HOME/lib/flink-connector-kafka-0.10_2.11-1.3.0.jar,$FLINK_HOME/lib/flink-dist_2.11-1.3.0.jar,$FLINK_HOME/lib/kafka-clients-0.10.0.0.jar,$FLINK_HOME/lib/jedis-2.9.0.jar,$FLINK_HOME/lib/commons-pool2-2.4.2.jar \
>/opt/MtdapProgram/mtdap3/flink-cluster/log/yarn-session.log 2>&1 &

二次识别套牌车
nohup bin/flink run \
--class com.enjoyor.mtdap3.realtime.SrFakeVehicle \
/opt/MtdapProgram/mtdap3/mtdap3-rtc-sr-vehicletrack-1.0/mtdap3-rtc-srfakevehicle-1.0.jar >/opt/MtdapProgram/mtdap3/mtdap3-rtc-sr-vehicletrack-1.0/log/mtdap3-rtc-srfakevehicle-1.0 2>&1 &

flink发布命令并没有指定yarn 怎么就在yarn上运行了?
配置一个容器,再发布一个flink程序  会自动找到 JobManager address (如果创建容器的时间和程序发布的时间间隔太久 会抛找不到jobManager address的异常) 使用 -yid 可以指定flink程序发布到 yarn cluster 集群上运行

flink  on yarn  两种运行方式 yarn session  single yarn job
=================AM RM==========================
Flink log中的 RM AM 指的是yarn上的一个ResourceManager 和若干个ApplicationMaster  指的是yarn的AM RM 通信
ApplicationMaster管理在yarn上运行的应用程序的每个实例
	同时负责协调来自 ResourceManager 的资源,并通过NodeManager监视容器的执行和资源使用（CPU、内存等的资源分配）

-------------
雅虎15年测试:
Storm 能够承受每秒 40 万事件,但受限于 CPU； 
Flink 则可以达到每秒 300万事件(7.5 倍)但受限于 Kafka 集群和 Flink 集群之间的网络

Flink 的执行过程是基于流的，这意味着各个处理阶段有更多的重叠,并且混洗操作是流水线式的,因此磁盘访问操作
更少.相反, MapReduce、Tez和Spark是基于批的,这意味着数据在通过
网络传输之前必须先被写入磁盘.该测试说明,在使用 Flink 时,系统空闲时间和磁盘访问操作更少。

接收器  数据源 

Kafka position也是由Flink自己维护的

理想下  无边际数据流 源源不断来  按照时间窗口 计算  输出

现实情况是: 数据不是按时来的 有延迟

所以划分为事件时间  摄取时间  处理时间

公司测试环境下的flink程序,运行26d了也没出现问题,原因在于flink使用的是自己的内存管理体系


===flink内存管理===
主流实时计算框架都是基于jvm语言开发的(Java Scala)
为了加快计算,通常都是将数据加载在内存中,由于数据量巨大,对内存造成很大压力
==数据存储==
最简单做法试封装成对象直接存储在List或Map这样的数据结构中(
	公司从mq中拿到的实时计算生产到的数据通过消费者程序写入到hbase 
	kafka  json  map  list(map) hbase)
引发两个问题?
1:数据规模大时,需要创建的对象非常多(数据加上存储的数据结构,耗费大量内存)
	可能引发OOM
2:源源不断的数据需要被处理,对象持续产生并需要被销毁
	GC压力大
SO:
JVM自带的GC无法满足高效+稳定的流处理,Flink建立一套自己的内存管理体系

Flink将内存分为3个部分(network buffers,Memory Manager pool,Remaining Heap)
每个部分都有不同用途;
1:Network buffers:一些以32KB Byte数组为单位的buffer,主要被网络模块用于数据的网络传输。
	在Flink中主要是基于Netty进行网络传输
2:Memory Manager pool大量以32KB Byte数组为单位的内存池,所有的运行时算法(例如Sort/Shuffle/Join)都从这个内存池申请内存，
       并将序列化后的数据存储其中，结束后释放回内存池
   内存池,由多个MemorySegment组成,每个MemorySegment代表一块连续的内存空间 byte[]数据结构存储  默认32kb
   
3:Remaining(Free)Heap主要留给UDF中用户自己创建的Java对象,由JVM管理.
	用在UDF中用户自己创建的对象 在UDF中,用户流式的处理数据 并不需要太大内存
	同时flink也不建议在UDF中缓存很多数据


重点:
Flink的主动内存管理避免了令人讨厌的OutOfMemoryErrors杀死JVM并减少垃圾收集开销的问题。
Flink具有高效的数据去/序列化堆栈，有助于对二进制数据进行操作，并使更多数据适合内存。
Flink的DBMS风格的运算符本身在二进制数据上运行，在必要时可以在内存中高性能地转移到磁盘。

===Lambda架构===
Lambda 架构用定期运行的批处理作业来实现应用程序的持续性，并通过流
处理器获得预警。流处理器实时提供近似结果；批处理层最终会对近似结果予以纠正

批处理架构很难解决乱序事件流问题
批处理作业的界限不清晰,写死了 假设需要根据产生数据的时间段(如从用户登录到退出)生成
聚合结果，而不是简单地以小时为单位分割数据


=flink流表对偶性=
流和动态表(Dynamic Table)的对偶(duality)性。


======流式SQL=====
SQL 声明式语言

批处理实例:
SELECT a.id FROM A a,B b WHERE a.id=b.id;	
	最简单的双流join,找出表A和表B中相同的id
两个经典的Join算法(归并连接 && 哈希连接):
归并连接算法:拿到两表后,将id由小到大排好序,然后从前往后同时遍历两表
				一旦遇到相等的id就输出一条Join结果(1,排序2,合并及连接)
哈希连接算法:拿到两张表,对数据规模就行一个评估,然后选取较小一张表
			 ,以id作为key,以数据行作为value,建立哈希索引.
			 接着便利另一张大表,对每一个id值到建立好的哈希索引中查找有没有id相等的数据行,
			 有则Join输出
区别:哈希连接算法中,只需要将较小的一张表加载到内存
	  归并连接算法,需要将两张表都加在到内存中
	  
	  
流式场景下的持续查询:
持续查询:没有外界干预的情况下,查询会一直输出结果,不会停止
由于数据流本身无穷,所以在上面的查询都是持续查询,每当有新数据的到来,可能都会有新的查询结果产生
	A B两张表双流Join ,表中数据不断增加,当A B存在相同id,才会生成相应的Join结果
问题:传统数据库允许对一张表进行全表扫描,但是流式场景却做不到
原因:1:我们无法确定下一条数据属于表A还是表B 2:流数据具有无尽性,找不到表的边界
所以传统的归并连接跟哈希连接算法不适用于流式SQL

-------------------Flink 流式SQL 提供多种Join

 


==========================
堆外内存(off-heap),堆内存(on-heap)
https://blog.csdn.net/u010722938/article/details/51558315

flink on yarn
Client提交App到RM上面去运行，然后RM分配第一个container去运行AM，然后由AM去负责资源的监督和管理。
需要说明的是，Flink的yarn模式更加类似spark on yarn的cluster模式，在cluster模式中，dirver将作为AM中的一个线程去运行，
在Flink on yarn模式也是会将JobManager启动在container里面，去做个driver类似的task调度和分配，
YARN AM与Flink JobManager在同一个Container中，这样AM可以知道Flink JobManager的地址，
从而AM可以申请Container去启动Flink TaskManager。待Flink成功运行在YARN集群上，
Flink YARN Client就可以提交Flink Job到Flink JobManager，并进行后续的映射、调度和计算处理。


批流是怎样统一的?
Batch和streaming会有两个不同的ExecutionEnvironment,不同的ExecutionEnvironment会将不同的API翻译成不同的JobGgrah,
JobGraph 之上除了 StreamGraph 还有 OptimizedPlan.OptimizedPlan 是由 Batch API 转换而来的.
StreamGraph 是由 Stream API 转换而来的,JobGraph 的责任就是统一 Batch 和 Stream 的图.

Flink JobManagerHA ?
与Storm不同的是，知道Storm在遇到异常的时候是非常简单粗暴的，
比如说有发生了异常，可能用户没有在代码中进行比较规范的异常处(至少执行一次)的语义，
比如说一个网络超时的异常对他而言影响可能并没有那么大，
但是Flink不同的是他对异常的容忍度是非常的苛刻的，那时候就考虑的是比如说会发生节点或者是网络的故障，
那JobManager单点问题可能就是一个瓶颈，JobManager那个如果挂掉的话，
那么可能对整个作业的影响就是不可恢复的，所以考虑了做HA




==flink学习==
flink人大视频 
https://www.bilibili.com/video/av42427050/




===Blink===
2019.2开源的Blink版本,主要是基于flink 1.5.1
看看就行 还是用Apache flink 


==========钉钉============
1.先对flink有宏观视野上理解，看阿里大会整理的文档资料，其中也有可运行的demo。
2.看各个flink 大V的博客，对架构和原理逐步深入理解（可以微信和google搜索关键词）
3.英文好，最好是看Apache flink官网文档，既全面又权威(先看前面后看官网，国人帮整理，理解速度会快些)
4.基于以上3方面理解基础上，自己有相当的技术基础及理解能力 + 毅力恒心 + 热爱程度，可以去啃下源代码  基本问题不大(往下又分2种情况 1.有基础曾经研究并扩展过开源代码  2.无基础 无经验  花费时间就要长些，需要毅力  有人指点会快些)
###从现象到本质再到解决方案  
比如这个:追踪flink上传的jar放到那个目录 : http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/

===告警===
Apache Committer 2017.7月 ali 5个   
参考:http://wuchong.me/blog/2019/02/12/how-to-become-apache-committer/ 


=================flink开发步骤==========
第一步:构建环境
val streamEnv = StreamExecutionEnvironment.getExecutionEnvironment
streamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
第二步:添加数据源
val prop:Properties = new Properties()
prop.setProperty("bootstrap.servers",kafkaProp.getProperty("bootstrap.servers"))
val consumer010= new FlinkKafkaConsumer010(kafkaProp.getProperty("source.topic")
				 ,new SimpleStringSchema(),prop)
consumer010.setStartFromLatest()
val dataStream = streamEnv.addSource(consumer010)
第三步:数据预处理
val outputStream = dataStream
.map(x=>getRecord(x))
.filter(!_._1.isEmpty)
.map(x=>recordProcess(x)) 
第四步:设置时间戳和水印
.assignTimestampsAndWatermarks(new TimestampExtractor(basicProp.getProperty("job.interval").toInt))
第五步:数据分组
.keyBy(0)
第六步:指定时间窗口+聚合计算+输出格式
.timeWindow(Time.seconds(basicProp.getProperty("max.lagged.time").toInt))
.reduce((v1,v2)=>(v1._1,v1._2,v1._3+v2._3,v1._4+v2._4))
.map(x=>toJson(x))
第七步:输出
outputStream.addSink(producer010)
第八步:执行flink
env.execute(basicProp.getProperty("application.name"))


=======================transformation========================
flink 常用转换算子:
map,flatMap,filter,keyBy,reduce,fold,aggregations,window,WindowAll,
Union,Window join,Split,Select,Project
dataSource.map(getRecord(_))
  .filter(new FilterFunction[(String, String, String, Long)] {
	  override def filter(t: (String, String, String, Long)): Boolean = {
		  t._4 match {
			  case t._4 if (t._4) > 20 => true
			  case t._4 if (t._4) <= 20 => false
		  }
	  }
  })
  /*上下等同,相当于是源码中类似这样已经实现*/
  .filter(_._4>20)
 
#### 调用filter算子()可以通过重写FilterFunction接口来实现 filter方法 
  
==================================Flink 双流转换================================
算子:coGroup join coflatmap

Join:只输出匹配成功的数据
CoGroup:无论是否匹配都会输出
CoFlatMap:没有匹配操作,只是分别接收两个流的输入

-------------------------
join,coGroup实现代码结构
val stream1 = ...
val stream2 = ...

stream1.join(stream2)
    .where(_._1).equalTo(_._1) //join的条件stream1中的某个字段和stream2中的字段值相等
    .window(...) // 指定window，stream1和stream2中的数据会进入到该window中。只有该window中的数据才会被后续操作join
    .apply((t1, t2, out: Collector[String]) => {
      out.collect(...) // 捕获到匹配的数据t1和t2，在这里可以进行组装等操作
    })
    .print()
--------------------------------

=============================================背压/反压======================================
参照:https://yq.aliyun.com/articles/64821
https://www.cnblogs.com/lanyun0520/p/5676617.html

背压
一个任务的back pressure警告(high),则意味着该任务产生数据的速度要高于下游Operator消化的速度
数据沿着job的数据流图向下游流动(如从source到sink),而背压则是沿着相反的方向传播,逆流而上,可以理解为 水流漫上去

Flink 如何处理背压?
Flink与持久化的source(例如kafka),能够为你提供即时的背压处理.而无需担心数据丢失.
Flink不需要一个特殊的机制来处理背压.因为Flink中的数据传输相当于已经提供了应对背压的机制.
因此,Flink所获得的最大吞吐量由其pipeline中最慢的部件决定




===parallelism  && slot====
parallelism 并行度 默认1 
程序中设置 env.setParallelism(3); 这里设置的是全局的,包含下面执行的每一个算子
为每一个算子单独设置并行度: dataStream.map(new XxxMapFunction).setParallelism(5)  
优先级:算子设置并行度 > env 设置并行度 > 配置文件默认并行度	
	eg:如果在代码中设置的是env.setParallelism(5),flink-conf.yaml文件中默认为1(1.4) 发布上flink-cluster 上会使用5个slot 


slot:作用,是指taskmanager的并发执行能力,简单理解是将CPU和内存分成一个个逻辑单位,即slot


================Flink算子====================

----------------------------数据流分组-----------------------------
keyBy 用于指定数据流是否进行分组
	需要在window函数前指定好,使用keyBy(...)可以将数据流拆分成逻辑分组的数据流
	如果不使用keyBy,你的数据流不是分组的
	分组数据流将你的window计算通过多任务并发执行,每一个逻辑分组流在执行中与其他的逻辑分组流是独立地进行的
在非分组的数据流中,你的原始数据并不会拆分成多个逻辑流并且所有的window逻辑都在一个任务中执行,并发度为1


#################################################################################################
由问题引入  window   time  watermark 

主要包括为什么要有 Window ?
Window 中的三个核心组件：WindowAssigner、Trigger 和 Evictor ?
Window 中怎么处理乱序数据, 乱序数据是否允许延迟, 以及怎么处理迟到的数据  ?
最后我们梳理了整个 Window 的数据流程, 以及 Window 中怎么保证 Exactly Once 语义  ?

----------------------------------------------window------------------------------
Flink 天然支持无限流数据处理的分布式计算框架,使用window将无限流切分成有限流,是处理有限流的核心组件

窗口分类: 
	时间驱动timewindow,数据驱动countwindow
相关Api:[] 不强制使用,有默认值
keyed windows
stream
	.keyBy(...)
	.window(...)
	[.trigger(...)]
	[.evictor(...)]
	[.allowedLateness(...)]
	[.sideOutPutLateData(...)]
	.reduce/aggregate/fold/apply()
	[.getSideOutput(...)]

non-keyed windows
stream 
	.windowAll(...)
	[.trigger(...)]
	[.evictor(...)]
	[.allowedLateness(...)]
	[.sideOutPutLateData(...)]
	.reduce/aggregate/fold/apply()
	[.getSideOutput(...)]

 
dataStream :
对于非KeyedStream，有timeWindowAll、countWindowAll、windowAll操作，其中最主要的是windowAll操作，它的parallelism为1，它需要一个WindowAssigner参数，返回的是AllWindowedStream
TimeWindowAll  不是并行的 所以slot只能用1 个,滚动窗口

KeyedStream:	对于KeyedStream除了继承了DataStream的window相关操作，它主要用的是timeWindow、countWindow、window操作，其中最主要的是window操作，它也需要一个WindowAssigner参数，返回的是WindowedStream


## Window API使用
WindowAssigner(窗口指定器)  Evictor(清除)   Trigger(触发) 

1:Window方法接收的输入是一个WindowAssigner,WindowAssigner负责将每条数据分发到正确的Window中(一条数据有可能会分发到多个窗口中)
几种通用的WindowAssigner:(tumbing window 滚动窗口 无重复 ,sliding window 滑动窗口 有重复,
							session window 事件窗口,global window 全局窗口)
## 自定义数据分发策略:新建一个class 继承WindowAssigner

2:Evictor 主要用于做一些数据的自定义操作,可选操作
CountEvictor 保留指定数量的元素
DeltaEvictor 通过执行用户给定的 DeltaFunction 以及预设的 threshold,判断是否删除一个元素
TimeEvictor 设定一个阈值 interval,删除所有不再 max_ts - interval 范围内的元素,其中 max_ts 是窗口内时间戳的最大值

3:Trigger 用来判断一个窗口是否需要被触发,每个 WindowAssigner 都自带一个默认的 Trigger
	允许自定义,继承 Trigger
	onElement() 每次往 window 增加一个元素的时候都会触发
	onEventTime() 当 event-time timer 被触发的时候会调用
	onProcessingTime() 当 processing-time timer 被触发的时候会调用
	onMerge() 对两个 trigger 的 state 进行 merge 操作
	clear() window 销毁的时候被调用
上面的接口中前三个会返回一个 TriggerResult
	CONTINUE 不做任何事情
	FIRE 触发 window
	PURGE 清空整个 window 的元素并销毁窗口
	FIRE_AND_PURGE 触发窗口，然后销毁窗口

flink window:
http://www.aboutyun.com/thread-26483-1-1.html
https://www.infoq.cn/article/WCOvi-D68Y8ycCiYZ8pX

flink触发当前窗口计算的前提是下一条数据的时间在当前窗口的结束时间之后

----------------------------------------------------------Time && Watermark----------------------------------------------------
前提:Time 和 Watermark适用于时间驱动类的窗口 

分布式环境中Time,事件时间(Event-Time),摄取时间(Ingestion-Time),处理时间(Processing-Time)

Event-Time:数据产生的时间
Ingestion-Time:数据进入到flink的时间
Processing-Time:窗口开始计算的时间
### env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); 

Watermark: 解决乱序的问题
如何保证基于事件时间的窗口销毁时,flink已经处理完所有数据?
Watermark会携带一个单调递增的时间戳t,watermark(t)表示所有时间戳不大于t的都已经到来
未来小于t的数据不会到来,因此可以放心触发和销毁窗口

# 迟到的数据 late elements
问题:
watermater的时间不好设置 :要么不正确 要么耗费太大 ,所以在设置watermark(t)之后,还有较小概率接收到时间戳(t)
之前的数据,这部分称为 late elements

解决方式:指定允许延迟的最大时间(默认0) 
DataStream<T> input = ... ;
input
	.keyBy(<key selector>)
	.window(<window assigner>)
	.allowedLateness(<time>)
	.<windowed  transformation>(window function)

设置allowedLateness之后,迟到的数据同样可以触发窗口,进行计算
利用Flink的side output机制,我们可以获取到这些迟到的数据

final OutputTag<T>  lateOutputTag = new OutputTag[T]("late-data"){};
DataStream<T> input = ... ;

SingleOutputStreamOperator<T> result = input
	.keyBy(<key selector>)
	.window(<window assigner>)
	.allowedLateness(<time>)
	.sideOutPutLateData(lateOutputTag)
	.<windowed  transformation>(window function)

DataStream<T> lateStream = result.getSideOutput(lateOutputTag);

# 设置allowedLateness之后,迟到的数据也可能触发窗口,如果使用的是Session window 
  可能会对窗口进行合并,产生预期外的行为  
 
3:Window 内部实现
每条过车数据进来后,会先由WindowAssigner 分配到对应的window(滚动/滑动..)
当window经过watermark被trigger后,会交给Evictor(如果没有设置 则跳过),
然后处理UserFunction(用户处理的逻辑代码)

###  Window中状态存储  
#Flink 是支持 Exactly Once 处理语义的,那么 Window 中的状态存储和普通的状态存储又有什么不一样的地方呢？
A:从接口上可以认为没有区别,但是每个 Window 会属于不同的 namespace,
而非 Window 场景下,则都属于 VoidNamespace ,最终由 State/Checkpoint 来保证数据的 Exactly Once 语义
简单说:Window 中的的元素同样是通过 State 进行维护,然后由 Checkpoint 机制保证 Exactly Once 语义




-------------------flink精确一次优势-------------------
Spark Streaming 的端到端 Exactly-once 需要下游支持幂等、上游支持流量重放，
Spark Streaming 这一层做到了 At-least-once，正常情况下数据不重不少，但在程序重启时可能
会重发部分数据，为了实现全局的 Exactly-once，在下游做了去重逻辑
通过快照机制,Flink 获得了端到端数据一致性


-------------怎样判断是否是新用户------------
跟判断是否首次入城车一样,redis 缓存数据库 永不过期  数据流实时对比判断  

Flink 现在大部分开发属于实时汇总 :  简单算子+业务函数+水印+分组+聚合+json格式输出


source  transformation  sink

------------------------------------ 怎样确定Flink集群所需资源 --------------------------------------

#吞吐量:
估算预期进入流计算系统每秒的记录数(吞吐量),以及每条记录数的大小

#不同key的数量以及每个key存储的state大小
key的数量和key所需state的大小,都将会影响Flink应用程序所需的资源.可以通过查看反压状态

#状态的更新频率和状态后端的访问模式:
不同的状态后端(RocksDB,java Heap)的访问模式差距很大.RocksDB每次读取和更新会进行序列化
反序列化以及JNI操作,Java Heap不支持增量checkpoint,会导致状态大的场景每次持久化的数据量很大
都会影响Flink作业所需的资源

#网络容量
网络容量不仅会受flink内部,也会受到Flink跟正在交互的kafka,hdfs等外部服务
比如启动kafka的replication会增加额外的网络容量

#磁盘带宽
如果应用程序依赖 RocksDB Kafka  HDFS

#机器数量以及可用的CPU和内存

另外需要提供额外的资源来保证:
当你的 Flink 发生故障时,系统会需要额外的资源来做恢复工作以及从 Kafka topic 或其他消息客户端追上最新的数据


-------------------------------------Flink指定 集群 发布---------------------------------
-yid + 目标applicationId

nohup bin/flink run \
-yid application_1559210378247_2887 \
-p 18 \
--class com.enjoyor.mtdap3.realtime.VehicleTrack \
/opt/MtdapProgram/mtdap3-flink/mtdap3-rtc-vehicletrack-1.0/mtdap3-rtc-vehicletrack-1.0.jar \
>/opt/MtdapProgram/mtdap3-flink/mtdap3-rtc-vehicletrack-1.0/log/mtdap3-rtc-vehicletrack.log 2>&1 &

--------------实时计算的趋势---------
实时任务SQL化
阿里的实时计算方案就是Flink SQL   1CU	一天6 RMB

------------ 源表  &&  维表  &&  结果表 --------------
源表: 流数据分析的源表是指流式数据存储，流式数据存储驱动流数据分析的运行  相当于ods

事实表&&维表   dwd
事实表:事实表是数据聚合后依据某个维度生成的结果表,一般很大
维表:对数据分析时所用的一个量,你可以选择按类别来进行分析,或按区域来分析. 这样的按..分析就构成一个维度

结果表: 数据统计后的结果        dws层

--------------   怎样确定集群所需资源? --------------
#吞吐量:
估算预期进入流计算系统每秒的记录数(吞吐量),以及每条记录数的大小
#不同key的数量以及每个key存储的state大小
key的数量和key所需state的大小,都将会影响Flink应用程序所需的资源.可以通过查看反压状态
#状态的更新频率和状态后端的访问模式:
不同的状态后端(RocksDB,java Heap)的访问模式差距很大.RocksDB每次读取和更新会进行序列化
反序列化以及JNI操作,Java Heap不支持增量checkpoint,会导致状态大的场景每次持久化的数据量很大
都会影响Flink作业所需的资源
#网络容量
网络容量不仅会受flink内部,也会受到Flink跟正在交互的kafka,hdfs等外部服务
比如启动kafka的replication会增加额外的网络容量
#磁盘带宽
如果应用程序依赖 RocksDB Kafka  HDFS
#机器数量以及可用的CPU和内存

另外需要提供额外的资源来保证:
当你的 Flink 发生故障时,系统会需要额外的资源来做恢复工作以及从 Kafka topic 或其他消息客户端追上最新的数据



------------------------华为大数据平台flink安全模式部署----------------


