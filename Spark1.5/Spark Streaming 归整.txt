Spark Streaming
任何的流处理系统,广义上分为 三个步骤:  接收数据  转换数据  输出数据

##容错##
Spark Streaming 怎样实现容错?
===基础准备====
Spark RDD 怎样实现容错的?
1:RDD 不可变的,确定的可重新计算的分布式数据集,内部会记录执行的操作
2:如果出现worker节点故障导致RDD任何分区丢失,可以从原始数据集重新计算该分区
3:如果RDD的每一步转换都是确定的,无论spark集群如何故障,最终转换后的结果总是相同的

Spark对提供容错的文件系统(HDFS/S3)中数据操作,计算生成的rdd也是容错的(大不了重新计算)

但是同样适用于SparkStreaming 吗?

否.大多数据的实时计算程序都是通过网络接收的数据,Spark Streaming如果想为所有生成的RDD
实现相同的容错属性:做法是接收到的数据将在集群中的Worker node中执行的多个Spark executor
之间进行复制(默认2),这样就导致如果系统发生故障需要恢复两种数据:
1:接收和复制的数据 此数据在单节点故障时仍然存在,因为其副本数据存在其他某一个节点上
2:接收到的数据在缓存上用于复制,但还未复制.恢复这部分的数据是从数据源中再次获取

还应该关注两种失败:
1:worder node 或者executor node 失败,这些node的所有内存数据都会丢失
如果接收器也在这些故障节点上,那么缓冲数据也会丢失
2:driver node失败 如果运行Spark Streaming 应用程序的驱动节点失败,很明显SparkContext会丢失,
所有执行程序和内存数据都将丢失
------------------------------------------

Spark Streaming的容错语义(Semantics)

流系统的语义通常根据系统处理每条记录的次数表示.(提供三种)  important!!!  一致性三个层级:

1:At most once : 每条记录将被处理一次或根本不处理
2:At least once : 每条记录将被处理一次或多次. 优于最多一次,起码不会丢失数据,但可能会有重复数据被计算
3:Exactly once : 每次记录只处理一次 ,不会丢失数据,也不会多次处理数据  最优

任何的流处理系统,广义上分为 三个步骤:  接收数据  转换数据  输出数据
转换数据:使用DStream 或 RDD转换接收到的数据

流应用程序必须保证端到端的一次性保证,每一步骤都必须提供一次性保证.
	即每条记录 只接受一次,只处理一次,只输出到下游系统一次
1:接收数据 不同的输入源保提供不同的保证
2:转换数据 由于RDD的保证,所有接收到的数据将只处理一次
			即使出现故障,只要接收的数据源可访问,最终转换的RDD将始终具有相同的内容
3:输出数据 默认情况下输出操作至少确保一次语义


先看接收操作:
1:HDFS文件:提供容错,Spark Streaming可以始终从任何故障中恢复并处理所有数据。
		这给出了一次性语义,这意味着无论失败什么,所有数据都将被处理一次
2:使用Receiver源:
	分为可靠的接收器和不可靠的接收器  ,节点故障 可靠不丢失 不可靠丢失
	为了解决这个问题,Spark 1.2引入了预写日志,将接收到的数据保存到容错存储中 语义上是至少一次
3:使用Kafka Direct API
	spark1.3增加 Kafka Direct API 它可以确保Spark Streaming只接收一次所有Kafka数据
	
输出操作: 提供至少一次 语义
	幂等更新  即多次尝试始终写入相同的数据
	事务性更新 
--------------------------------------
Spark streaming的at least once是如何实现的? 

Spark streaming的每个batch可以看做是一个Spark任务,
1:receiver会先将数据写入WAL,保证receiver宕机时,从数据源获取的数据能够从日志中恢复(注意这里,早期的Spark streaming的receiver存在重复接收数据的情况),
并且依赖RDD实现内部的exactly once(可以简单的理解采用批量计算的方式来实现)
RDD: Resilient Distributed Dataset 弹性分布式数据集
Spark保存着RDD之间的依赖关系,保证RDD计算失败时,可以通过上游RDD进行重新计算
---------------------------------------







